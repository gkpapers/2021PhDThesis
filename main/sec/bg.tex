We will first define and discuss the state of reproducibility in science across a range of domains, and explore several
specific examples. Next, we will discuss the role that software re-executability and numerical stability play in these
problems, and advances which have been made in each of these spaces. We will then approach neuroimaging, with a brief
overview of the field and commonly used methods, finally presenting several case studies in neuroimaging to demonstrate
the relevance of exploring reproducibility and numerical stability in this space.

\subsection{Reproducibility}
At the heart of science is the ability for researchers to build upon previous work, incrementally advancing a given
field of research~\tocite \tmp{(some subset of refs 1-6 from open science collab)}. Despite being accepted as a cornerstone for progress, there is not a fully unified definition around
this practice, or the lack thereof in some cases~\tocite (https://doi.org/10.3389/fninf.2017.00076, https://www.nature.com/articles/s41562-019-0629-z, https://stm.sciencemag.org/content/8/341/341ps12.full).
In practice, distinct definitions have emerged to serve specific communities tackling issues of reproducibility, such
as The Association for Computing Machinery which defines reproducibility as the ability to re-obtain a measure with
stated quality and precision using an identical experimental setup when carried out by a different team in a different
location~\tocite(https://www.acm.org/publications/policies/artifact-review-badging). In the case of life sciences,
this definition has limited use since the equivalence of samples being measured (e.g. humans in a different city) or
equipment being used (e.g. equivalent equipment from two different manufacturers) is difficult or impossible to
quantify. In these cases, a distinction in the level of reproducibility is often drawn with milestones corresponding to
the ability to reproduce the methods used, the ability to obtain the same or equivalent results, and the ability to
draw equivalent inferences from the results obtained~\tocite (https://doi.org/10.3389/fninf.2017.00076).

In an effort to increase clarity around this topic and handfuls of overlapping definitions, there have been recent
efforts to visualize the intentions and slight conceptual differences across each~\tocite (https://www.nature.com/articles/s41562-019-0629-z) \tmp{add figure}.
In the paper referenced here, a distinction is made from reproducibility and replicability. In this case,
reproducibility is defined as the ability to exactly re-run the analysis using the existing data and tools, but a
unique analyst. Replicability then describes the ability to successfully re-do the experiment, including new data
collection, experimenters, and software, but ultimately arrive at the same claim. These definitions have practical
value in the life sciences as they allow for the differentiation between a claim remaining unchanged in either the same
or differing experimental configurations and equipment. These definitions, often referred to colloquially as ``Peng's
Reproducibility'', will be the definitions referred to throughout this thesis.

In addition to the definitions of reproducibility and replicability accepted above, I will refer to an additional term
in this space: re-executability, which simply refers to the ability of being able to ``hit go'' on the analysis
subsequent to its original execution. This definition closely matches a definition of reproducibility mentioned above,
but no analog exists within the Peng framework, so it is added here for clarity.

With a rich and growing space of conceptual frameworks through which re-executability, reproducibility, and
replicability can be evaluated, it is perhaps self-evident that the level of trustworthiness across many disciplines
has recently become a topic of interest. I will now introduce the so-called ``Reproducibility Crisis'', an umbrella
phrase which captures this movement, and highlight its relevance in both psychological and neurological sciences,
serving as motivation for work carried out as a part of this thesis.

\subsubsection{The Reproducibility Crisis}

The reproducibility of findings has long been a topic of concern to researchers in the life sciences~\tocite (https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124, https://www.nature.com/articles/483531a?linkId=33568136, https://www.nature.com/articles/nrd3439-c1?linkId=33568131, https://science.sciencemag.org/content/343/6168/229.full).
However, several recent initiatives have brought this somewhat niche question into an area of broad concern. In 2015,
the Open Science Collaboration~\tocite (https://science.sciencemag.org/content/349/6251/aac4716) organized an attempted
replication of $100$ recent research papers in psychology~\tocite. Their result, showing that approximately two-thirds
of the studies failed to replicate (\tmp{add figure}), was swept up by mainstream media and proclaimed a crisis.

In an effort to characterize the so-called crisis, Nature conducted a survey of $1,500$ scientists across a wide
range of disciplines, probing their beliefs about the reproducibility of science in their field, and found that $90\%$
of respondents felt that their field was in crisis~\tocite (https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970).

With this topic now in the public eye, studies continued to explore various causes in attempt to understand and correct
the widely shared concern. Unsurprisingly, this problem was tackled from a number of different directions, even within
a given discipline. In neuroscience, statistical power has long been believed to be a culprit for
irreproducibility~\tocite (https://www.nature.com/articles/nrn3475); a meta-analytic study of the power of findings in
literature estimated a median power of findings at $21\%$. In practice, this work assumed the robustness of data and
methods used throughout the studies, and performed a purely statistical evaluation.

In neuroimaging, studies began to dive deeper into intermediate methods and explored their compounded effect on
null hypothesis significance testing. A study evaluating the accuracy of significance testing frameworks within
commonly used libraries for analysis of functional MRI (fMRI) data found that under certain conditions false-positive
rates were as high as $70\%$~\tocite (https://www.pnas.org/content/113/28/7900). While the presentation of this result
may misleadingly construe an inflated significance of these errors (see associated letters~\tocite and
revisions~\tocite), an undeniable attribute of these findings which is the often-dramatic difference between software
libraries. The impact that these differences have on analytical workflows was summarized well in https://onlinelibrary.wiley.com/doi/full/10.1002/hbm.24603),
which showed that not only do the tools produce qualitatively different results, but could sometimes lead to diverging
conclusions.

In all of the cases above, analyses were chosen to be evaluated explicitly because they could be re-executed across
slightly different conditions.  While all published methods, such as software written to produce a given result, would
continue to function in an ideal world, in practice this is not the case. A study exploring published papers in
computer science found that of $600$ papers which were analyzed, approximately only $200$ were able to weakly
replicate~\cite{collberg2016repeatability}. In many cases, this was due to
inadequate description of the specific implementations, environments, or how tools were applied, while in others, it
was likely due to the commonly accepted phenomenon of ``Software Aging''~\cite{parnas1994software}. Regardless of the
specific causes, this inability to re-execute workflows makes claims derived from these tools irreproducible at the
most fundamental level. Prior to exploring possible numerical underpinnings behind irreproducibility, it is essential
to ensure the re-executability of analyses. The following section will discuss advances made prior to this thesis which
facilitate the construction and distribution of re-executable scientific workflows.

\subsection{Software Re-executability}
There are a number of factors which limit the re-executability scientific workflows, including incomplete specification
of processing detail or lack of data and tool availability~\tocite (https://www.nature.com/articles/ng.295?cacheBust=1510009844072).
While the public sharing and dissemination of data may not always be possible in the life sciences due to concerns
around ethics or the informed consent of participants~\cite{ross2018ethical,duke2013ethics}, there exist practical
guides which can help researchers overcome this hurdle in certain cases (e.g. for
neuroimaging~\cite{brakewood2013ethics} and psychology~\cite{meyer2018practical}). Given that ethical barriers should
not exist limiting the reusability of software, this section will focus on tangible efforts which have been made to
increase the re-executability of analyses.

\subsubsection{Virtual Environments}
The configuration of tools and software environments is a necessary step prior to the execution of computational
workflows. While authors set up their computational environments prior to carrying out experiments, these
configurations are often left undocumented in published manuscripts, leaving readers with the daunting task of
determining what versions of libraries were used and their dependencies~\cite{robles2010replicating}. However, this is
not only an inconvenience but becomes scientifically troublesome when considering the reality that distinct software
versions lead to different interfaces and results by design~\cite{raymond1997cathedral}. In programming languages such
as Python~\cite{oliphant2007python}, packages may contain information strictly defining their requirements. However,
these definitions are limited to capturing the dependencies written within the same language, and are often written
with version ``bounds'' (i.e. requires package X with version $\geq 1.0$), to simplify the interoperability between
libraries.

Traditionally, this issue has been solved through the use of Virtual Machines (VMs)~\tocite. VMs are a method for
packaging and sharing tools that are pre-installed in an executable environment which can be run across
machines. A drawback of these machines is that they are often require considerable storage and computational overhead,
as they contain complete system operating systems which must be launched with dedicated processors and memory, limiting
their portability. Over the last few years, techniques for the virtual encapsulation of tools have shifted towards a
``Container'' approach, which do not suffer from similar overheads. Container environments, made accessible primarily
through Docker~\tocite and Singularity~\tocite, rely on the kernel and other libraries of the host operating system,
leading to smaller bundles that are less computationally expensive to run. While containers are not a perfect solution
that guarantee re-executability\footnote{Due to problems which may arise by violating some of the best-practices
described here: \url{https://developers.redhat.com/blog/2016/02/24/10-things-to-avoid-in-docker-containers/}}, they are
an important advancement which greatly increases the ability to distribute and consume scientific software.

\subsubsection{Tool \& Analysis Descriptions Standards}
Beyond tool configuration, the most fundamental components of re-executability are a clear characterization of the
tool(s) being run and the arguments or inputs provided to them. Despite this, there is no universally accepted
description standard for software. In practice, grassroots standards often emerge by necessity in a given domain, and
gain local adoption. In the case of neuroimaging, such standards include Brain Imaging Data Structure applications
(BIDS apps)~\tocite and Boutiques~\tocite, each of which takes a distinct approach to this problem.

The Brain Imagine Data Structure defines an organization standard for neuroimaging datasets, with the goal of lowering
the barrier to their sharing, interpretation, and inter-operation~\tocite. Accordingly, the BIDS app standard emerged
as a \textit{prescriptive} standard that dictates how tools should interact with these datasets~\tocite. This standard
requires that a set of base arguments must be supported and accepted by tools: the location of the dataset, the
location at which outputs should be placed, and the analysis level (e.g. individual-level versus group-level). This
standard also prescribes several optional arguments, such as allowing for the indication of a subset of subjects
selected for analysis. However, a key limitation of this standard is that beyond the initial prescription of arguments
there is no definition or description of subsequent arguments that may be relevant for a given tool, or standards for
provenance which will be subsequently recorded. The BIDS app standard dramatically simplifies performing an analysis
pipeline with default behaviour, but this lack of additional description makes it an incomplete solution for software
re-use when implemented at the barest level.

Boutiques facilitates re-executability through a \textit{descriptive} standard which requires software developers to
create a rich metadata record summarizing the arguments and function of their tool~\tocite. This standard supports a
wide range of command-line tools, and is supported by a library which directly manages the execution of described tools
alongside validation of inputs (e.g. ensuring two mutually-exclusive options are not provided) and provenance capture.
However, while the added complexity of Boutiques leads to more rich descriptions and facilitates more complete tool
re-use, this complexity also makes the standard considerably less accessible.

Other standards have been developed around neuroimaging which promote the capture of rich execution records, such as
the NeuroImaging Data Model~\tocite, or foster the construction of workflows and the distributed execution of tools,
such as CBRAIN~\tocite, LONI Pipeline~\tocite, and Nipype~\tocite. While these tools provide a rich set of features
and techniques to improve the re-executability of software, prior to this thesis there was no single tool in the
neuroimaging space which harmonized software virtualization, tool descriptions, and provenance capture that supported
the iterative development and deployment of analysis pipelines using command-line tools.

\subsection{Stability}

\begin{itemize}
\item the concept of stability
\item conditioning
\item forward versus backward stability
\item precision as a proxy when accuracy is not available
\end{itemize}

\subsubsection{Numerical Analysis}
\begin{itemize}
\item floating point arithmetic (MULLER, Jean-Michel, BRISEBARRE, Nicolas, DE DINECHIN, Florent, et al. Handbook of floating-point arithmetic. Birkhäuser, 2018.)
\item stochastic arithmetic \url{http://eprints.maths.manchester.ac.uk/2763/1/paper.pdf#page3}
\item MCA
\item significant digits as a measure of precision
\end{itemize}

\subsection{Neuroimaging}

\subsubsection{Diffusion MRI Processing}
\begin{itemize}
\item concept
\item modalities of interest: diffusion and structural
\item pipelines
\item strengths and weaknesses
\item network analysis/applications
\item \url{https://link.springer.com/article/10.1007/s00429-020-02129-z}
\item \url{https://www.pnas.org/content/111/46/16574}
\item \url{https://autofq.org/bibliography/diffusion-mri/}
\end{itemize}

\subsubsection{Evidence of Instability in Neuroimaging}
\begin{itemize}
\item (OSes, NARPS, tractograms, CoRR, One Voxel, Cluster Failure, ...)
\item Bias-variance tradeoff https://ieeexplore.ieee.org/document/824819
\item Arno registration comparison
\item \url{https://www.frontiersin.org/articles/10.3389/fnins.2015.00018/full}
\item \url{https://www.biorxiv.org/content/10.1101/2020.10.07.321083v1.full.pdf}
\end{itemize}

