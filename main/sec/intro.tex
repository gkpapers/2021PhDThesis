In an age of big data and an ever growing landscape of –omics, the structure and function of the brain is being
increasingly explored through connectomics. Brain maps, called connectomes, are constructed as networks in which areas
of the brain are related to one another through properties of interest~\cite{fornito2015connectomics}, such as their
morphology, connectivity, or function. These areas, also called regions of interest or vertices, and the properties or
edges connecting them are intentionally flexible, allowing the brain networks to describe a limitless range of scales
and organisms. While the connectome derived from a section of mouse brain may summarize the synapses between neurons
found in an Electron Miscropy image~\cite{zingg2014neural}, the construction of a similar map from humans
\textit{in vivo} is limited to data collected through non-invasive imaging techniques, primarily Magnetic Resonance
Imaging (MRI), and may summarize the global connectivity of the
brain~\cite{sporns2013human,van2012future,hagmann2005diffusion}.

Though the resolution of a typical MRI is approximately $1,000,000 \times$ less precise than an Electron Microscope
image~\cite{kasthuri2015saturated} which may show true cellular connectivity, MRI techniques allow for the capture of
entire brains without harming the subject and can be measured over time to observe network evolution. While regions
would be defined in the MRI case as macro-scale brain areas (often $1mm^3$ or larger in
size~\cite{sotiropoulos2013advances}), the edges between them can be defined as similarity in
structure~\cite{zielinski2010network,alexander2013convergence,montembeault2012impact},
function~\cite{zuo2012network,cao2014topological,kelly2012characterizing}, or
connectivity~\cite{sporns2005human,ingalhalikar2014sex,sporns2013human}. Exploring these networks can inform the
development of biomarkers~\cite{alexander2013convergence,ingalhalikar2014sex}, models of disease
progression~\cite{shah2017altered,yan2018rich}, and potentially identify targets for medical
intervention~\cite{silasi2014stroke,iturria2017multifactorial}.

In recent years, several large consortia such as the UK BioBank~\cite{sudlow2015uk}, the Human Connectome
Project~\cite{van2013wu}, and the Consortium of Reproducibility and Reliability~\cite{zuo2014open} have made it their
mission to capture and share brain imaging data from thousands of individuals which can be used to construct maps and
explore the brain. However, connectomes are not an automatic result of these images. The estimation of brain networks
relies on complex image processing software tools and scientific
pipelines~\cite{hagmann2005diffusion,esteban2019fmriprep}, including the denoising and alignment of images, tissue
classification and segmentation, and modelling or relating connectivity or function across regions. The orchestration
and design of these pipelines has considerable impact on the derived maps and their
application~\cite{bowring2019exploring,klein2009evaluation}. In the absence of ground-truth to evaluate the accuracy of
results generated from these tools, it is essential to understand their robustness to minor perturbations (e.g.
small amounts of noise). This consistency (or possible lack thereof) has recently become under scrutiny across brain
imaging~\cite{glatard2015reproducibility,Lewis2017-ll,eklund2016cluster}, necessitating a shift in the focus of
researchers with an increasing emphasis on reproducibility.

While findings may on occasion be irreproducible as the result of p-hacking (i.e. the modification of analyses in
search for significant results), it is often due to much more innocent means such as an inability to re-execute a
previous workflow~\cite{collberg2016repeatability}, software errors~\cite{eklund2016cluster}, system
upgrades~\cite{salari2020file}, or the underlying stability of algorithms~\cite{Lewis2017-ll}. While the existence of
these problems is becoming increasingly understood and accepted, their impact on the validity scientific claims or
models in neuroimaging has remained largely uncharacterized.

The objective of this thesis is to understand the role that instability plays in the reproducibility of results, and
develop methods around this exploration which enable higher quality and more easily reproducible claims in
neuroimaging. To this end, I have:

\begin{enumerate}[label=(\roman*)]
\item developed a software library which facilitates and records provenance for the parallel execution,
re-execution, visualization, and error detection of neuroimaging pipelines and datasets in high performance computing or cloud environments;
\item developed and evaluated various methods for perturbing pipelines and observing the numerical instabilities
inherent to structural connectome estimation;
\item quantified the impact of numerical instability on a set of neuroimaging analyses measuring absolute change,
dataset reliability, network topology, and the robustness of a brain-phenotype relationship; and,
\item improved the quality and generalizability of modelling a brain-phenotype relationship through the aggregation of
connectomes in a perturbation-augmented dataset.
\end{enumerate}

Ultimately, in collaboration with my co-authors, I created a piece of computational infrastructure which was used to
facilitate the execution of pipelines consuming several CPU-decades for the induction of numerical instabilities in
neuroimaging pipelines. I characterized the significant impact of these instabilities in various analytic contexts, and
proposed a method for leveraging instabilities in machine learning applications which improves the generalizability of
learned models. This thesis not only sheds light on this impactful issue in neuroimaging, but it presents a method for
further exploration on the trustworthiness of scientific tools and results more generally. My work also demonstrates
how scientific workflows can immediately benefit from explorations of numerical stability by leveraging the variability
inherent to workflows to reduce bias in models of brain structure. This thesis is manuscript based, meaning that each
of the four body-chapters is an exact copy of either a published or under-review manuscript.

\subsection{Contributions to Original Knowledge}
I have developed and contributed to several tools, methods, and experiments which increase the accessibility of
deploying and evaluating neuroimaging pipelines at scale. I have demonstrated the utility of these tools to explore and
characterize the stability of neuroimaging pipelines. Below, I summarize original contributions in each of these areas.

\subsubsection*{Software Contributions}
Extending the Boutiques command-line descriptive framework~\cite{Glatard2018-tu}, for which I am a co-maintainer, I
developed Clowdr~\cite{Kiar2019-sr} to enable the rapid deployment of scientific workflows across cloud and high
performance computing resources. This tool is publicly available and has effectively been used on the Amazon Web
Services cloud, Compute Canada, and Dell EMC resources, orchestrating decades of compute cycles over the resources in a
matter of hours. As workflows in neuroimaging often rely on prebuilt and containerized dependencies through Docker or
Singularity, I created Fuzzy (\href{https://github.com/verificarlo/fuzzy}{https://github.com/verificarlo/fuzzy}),
a curated collection of scientific libraries which were recompiled and instrumented to allow the stability evaluation
of the contained tools. These environments use Verificarlo~\cite{Denis2016-wo} to instrument libraries with Monte Carlo
Arithmetic~\cite{Parker1997-qq}. The precompiled libraries include: Python, Cython, BLAS, LAPACK, Libmath. The efficacy
of the perturbations induced through these tools has been demonstrated for neuroimaging applications through several
experiments mentioned in the following paragraph.

\subsubsection*{Scientific Contributions}
The software contributions above were developed out of necessity for the exploration of the stability of neuroimaging
analyses. First, I created and demonstrated the Fuzzy environments as an effective method for inducing instabilities in
neuroimaging pipelines, and situated these instabilities with respect to other forms of
perturbation~\cite{Kiar2020-lb}. In this paper, I demonstrated the considerable variability present in a structural
connectome estimation pipeline solely due to numerical uncertainty. I applied this technique to study the stability of
a set of typical network neuroscience experiments and characterized the effect of instabilities on each of a
test-retest, network topology, and phenotypic classification setting~\cite{Kiar2020-kz}. This work illustrates the
significant impact that numerical instabilities play in all levels of downstream analysis, spanning the reliability of
comparisons across subjects, to the lack of reliability in individual network features, and ultimately the modelling of
relationships between connectivity and phenotypic information (in this case, Body Mass Index). The final chapter of my
thesis focused on the aggregation of unstable derivatives and their impact on the performance and generalizability of
machine learning classifiers tasked with learning brain-phenotype relationships. The findings of this paper showed that
dataset augmentation through Monte Carlo Arithmetic leads to the development of more stable and performant classifiers,
and has significant implications on the development of robust neuroimaging biomarkers, as well as the potential to
increase the benefit gained from additional data collection. Together, these contributions bridge the gap between
numerical analysis and neuroimaging, providing novel insights into the reliability of tools, their results, and their
ultimate application in understanding brain structure.

\subsection{Contributions of Authors}
I was responsible for the experimental design, data processing, analysis, interpretation of results, and the majority
of writing for each manuscript. Specific co-author contributions to each chapter are summarized below.

\subsubsection*{Ch.I – A Serverless Tool for Platform Agnostic Computational Experiment Management}
I designed and developed the tools, experiments, and figures, and wrote the majority of the manuscript. Shawn T. Brown,
Tristan Glatard, and Alan C. Evans supported the design and development processes, edited the manuscript, and provided
valuable feedback. Tristan Glatard and Alan C. Evans jointly supervised this project.

\subsubsection*{Ch.II – Comparing Perturbation Models for Evaluating Stability of Neuroimaging Pipelines}
I was responsible for the experimental design, tool development, data processing, analysis, interpretation, and the
majority of writing. All authors contributed to the revision of the manuscript. Pablo de Oliveira Castro, Pierre Rioux,
Eric Petit, and Shawn T. Brown all provided software development support. Alan C. Evans and Tristan Glatard supported
the development process and jointly supervised this project.

\subsubsection*{Ch.III – Numerical Instabilities in Analytical Pipelines Lead to Large and Meaningful Variability in Brain
Networks}
I was responsible for the experimental design, data processing, analysis, interpretation, and the majority of writing.
All authors contributed to the revision of the manuscript. Yohan Chatelain, Pablo de Oliveira Castro, and Eric Petit
were responsible for Monte Carlo Arithmetic tool development and software testing. Ariel Rokem, Gaël Varoquaux, and
Bratislav Misic contributed to experimental design and interpretation. Tristan Glatard contributed to experimental
design, analysis, and interpretation. Tristan Glatard and Alan C. Evans were responsible for supervising and supporting
all of my contributions.

\subsubsection*{Ch.IV – Data Augmentation Through Monte Carlo Arithmetic Leads to More Generalizable Classification in
Connectomics}
I was responsible for the experimental design, data processing, analysis, interpretation, and the majority of writing.
All authors contributed to the revision of the manuscript. Yohan Chatelain and Ali Salari provided feedback on the
experimental design. Tristan Glatard and Alan C. Evans contributed to experimental design, analysis, interpretation,
and jointly supervised this project.

\clearpage
