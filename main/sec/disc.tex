The work presented across the manuscripts contained within this thesis demonstrate the creation and application of
infrastructure and methodology for the analysis of stability in neuroimaging pipelines, and demonstrate its
application for both the study of robustness in an analytical context and the generation of more generalizable
brain-phenotype relationships. The four component manuscripts incrementally present the methods developed to
perform a large scale evaluation of numerical stability in neuroimaging, each providing a foundation for the
following studies. As a result, this thesis has led to the creation of both scientific software resources and
considerable scientific advancement in the study of numerical stability in neuroimaging.

In the following sections I will discuss and interpret the findings as a whole and their implications on the field
of structural connectomics. I will then discuss the relevance of extrapolating these conclusions to other domains
of neuroimaging, including the analysis of both functional and structural images. Beyond discussing the
implications of the results presented here, I will propose ways in which perturbation analyses may be further
adopted in neuroimaging, and conclude with lessons learned both from my own pursuit of these projects and
recommendations for the field at large.

\subsection{The Impact of Perturbing Structural Connectome Estimation}

Throughout this thesis I have demonstrated that controlled perturbations serve as a viable and effective method
to study the stability of structural connectome estimation pipelines. In each of the various methods tested, such
as one-voxel noise injection, sparse Python-only Monte Carlo Arithmetic (MCA), or dense Full-stack MCA, we noticed
that the distinct nature of the perturbations in each case led to distinct variance in the results. In the case of
one-voxel perturbations, a small and localized form of noise, we noticed similarly localized changes in outputs.
However, the MCA instrumentations led to outcomes with local, topographical, or scaling changes across different
simulations of the same data. Perturbations with MCA were not only able to encapsulate similar effects and
deviations as were observed with the one-voxel methods, but the independence of the MCA method from the dataset,
the lack of introduced bias being introduced, and the global perturbation of operations makes MCA a more
scalable and generalizable technique. While I recommend the adoption of the MCA technique generally, one-voxel
methods can serve as a valuable method for the local evaluation of instabilities, such as an evaluation of the
sensitivity of an algorithm in a region that is difficult model, or in proximity to unavoidable contrast changes
such as in the presence of a lesion or significant atrophy.

The two MCA environments used throughout this thesis differed considerably in both their construction and the
resulting behaviour of tools. In the sparse setting, referred to across the manuscripts as either ``Python-only''
or ``Input Instrumentation'', a considerably smaller set of libraries were instrumented with MCA. In this case,
all floating point operations performed directly by Python or Cython-compiled code were perturbed, however, while
the widely adopted NumPy~\cite{harris2020array} library is a Python-installed module, it is largely
uninstrumented as operations therein predominantly rely on lower-level libraries BLAS~\cite{lawson1979basic} and
LAPACK~\cite{anderson1999lapack}. In the dense setting, predominantly referred to as ``Full-Stack'' or ``Pipeline
Instrumentation'', BLAS, LAPACK, and NumPy were instrumented as well. This distinction meant that the operations
perturbed in the sparse configuration were not those doing the ``heavy lifting'' of image, matrix, or array
manipulation, but only those doing more simple manipulations of data in between these operations. This results in
an important change to the MCA method as it was originally defined by Parker~\cite{Parker1997-qq}: the
few-and-far-between nature of perturbations both limits the law of large numbers~\cite{hsu1947complete} and
allows for the cascading of single perturbations without correction, and it is thus likely that there exists
numerical bias in perturbations introduced in the spare implementation as a result. The realizations of this bias,
however, is random across executions, such that an unbiased sampling of the distribution of results may still be
possible.

The observation that the bounds of variability were similar across the two MCA instrumentations, alongside
considerably reduced computational overhead in the Python-only case, suggests that sparsely applied MCA could be a
viable technique for approximating the bounds of pipeline variation as measured with dense (or, true) MCA. Just as
the list of instrumented libraries in the sparse configuration is a strict subset of those instrumented in the
dense case, it holds that the set of possible perturbed results in the sparse configuration is a strict subset of
the possible results in the dense case. This relationships is potentially powerful as this means that problems can
undergo a preliminary and less computationally intense stability evaluation prior to being evaluated more densely.
The coherence of results across the two perturbation configurations presented throughout Chapter III suggest that
the sparse Input Instrumentation is a high quality estimator of the true MCA distribution. Figure~\ref{ch3f2}
shows that the sample distributions of network statistics derived from the perturbed connectomes are unchanged
across both MCA configurations, and Table~\ref{ch3t1} demonstrates the ability of both techniques to increase the
reliability of the dataset. Uniquely, the connectomes resulting from the sparse MCA instrumentation contained less
session- and acquisition-dependent signal than either the original dataset or the true MCA results. While the
specific cause of this is result unknown, it is impossible to make a statement on the generalizability of the
effect, however, given its potential significance in brain imaging, it is an area of extreme interest for future
work.

The variability introduced by MCA also had considerable impact on the modelling of brain-phenotype relationships.
In the case of both MCA instrumentation methods, Figure~\ref{ch3f3} shows how random sampling of the perturbed
networks led to a distribution of performance on a BMI classification task spanning an approximate range of
$\pm 0.10$ relative to reference. The balance of performance about the original values lends credence to the
interpretation of MCA derived connectomes as being sampled from the distribution of equally plausible results,
and the unperturbed connectome is simply a point-estimate of this distribution. When exploring how to capture
the added variance across...

- Knowing that the perturbed results could lead to better brain-phenotype models, our aggregation efforts focused on
capturing the introduced variance in a variety of ways. We found that regardless of the technique, capturing this
added variance improved the performance of models and often led to models which were more generalizable on the
held-out datasets.

- These experiments show utility for perturbation analysis in neuroimaging both as a tool for the diagnosis of
software instabilities and a method for obtaining variance estimates on results which can be used to form richer
models.

- Other efforts which have explored the impact of analytical flexibility on findings sample a broad and rich space
that is not directly comparable to the techniques demonstrated here. In fact, combining these approaches would lead
to not only tool-specific characterizations of stability, but lead to a far richer capture of all possible
variability in results when asking a specific question.

\subsection{Extrapolating Conclusions}
- This thesis focuses on the analysis of diffusion MRI data towards the construction of structural connectomes, and
therefore does not make any strict claims about the relevance of these findings on other domains of neuroimaging.

- However, I will discuss here the components which likely play a role in the state of numerical stability, and how
these components were resolved in the case of the analyses evaluated here, and a comparison drawn between the
equivalent components in the context of other neuroimaging workflows and modalities.

- First up, data: signal to noise ratio, dimensionality, image contrast, all play a role on stability. These are
relatively consistent across all MRI modalities. Though higher SNR in structural imaging would in theory lead to
more robust results/less instability, if methods have been developed with underlying assumptions about the SNR then
the associated tools may in fact be less stable in the face of perturbation.

- Next, software quality: the tools used are based on published literature and software implementations are open
source, using commonly used libraries for all numerical operations that are adopted in all industries (i.e. numpy,
scipy). This is perhaps the best case, the quality of code is obvious, it is thoroughly tested, and it is public
and therefore has a high likelihood of benefitting from self correction. In the case of fMRI, this is the case for
some libraries, however, for much of functional and structural MRI it is common that analytical tools have been
developed in distinct silos, and have solved many difficult problems in isolation, each with their own set of
distinct strengths and weaknesses. It is likely that because of this, tools will be very heterogeneous from one
another, and the lack of complete adoption of public libraries makes errors more likely in these settings.

- Finally, difficult of modelling tasks: All are hard, DWI is likely among the easier conceptual modelling
frameworks, and produces much lower-dimensional results than structural surfaces, for example. It is conceptually
difficult to compare the fundamental difficulty (and therefore fundamental condition) of solving each domain
independently, but it is likely that the more high-dimensional a derivative in a given domain, the less stable it
will be given the increased complexity of estimation and unchanging input signal.

- Across arbitrary domains, a set of similar comparisons could be made, but the a priori assumption, if one had to
be made, should be that the higher quality the data, the more open the code, and the more simple the model, the
more stable the solution. This recommendation towards simplicity is not unlike the recommendation that ``linear
regression is a sufficiently descriptive model for solving most problems''.

\subsection{Future Directions in Operationalizing Perturbation Analysis}
- friendly reminder, we don't just need to think about one way to interpret perturbed results or only perturb
pipelines at a single portion of their execution. In fact, the flexibility of this method is precisely what makes
it so valuable.

- we'll discuss different components of workflows which would benefit from instrumentation in neuroimaging, as well
as identify possible applications of the results of these perturbations. Given that the intersection of numerical
analysis and neuroimaging is in its infancy, many of these potential explorations remain as open questions that an
interested reader could explore (or build a grant program around).

\subsubsection{Applications in Neuroimaging}
- Acquisition: initiatives such as gadgetron provide scanner-agnostic image reconstruction methods in an effort to
both improve the quality of these reconstructions and reduce cross-manufacturer variances. The perturbation of
reconstruction could be performed to identify which sequences are the most consistently robust across a set of
manufacturers. Higher fidelity images could be generated through the ensemble of perturbation-reconstructed images,
possibly achieving benefits like those found in HDR image enhancement.

- Image processing and modelling: images typically undergo numerous alignments, corrections, denoising procedures,
and phenomenological modelling. These processing steps, such as image registration which is relevant across all
modalities of neuroimaging, are often known to solve poorly conditioned problems and exhibit sensitivity to initial
conditions or noise. The application of perturbation analyses in these settings could be similarly used to either a)
test the bounds of performance on the function, or b) be performed iterately so a stable solution can be estimated
and applied.

- Machine Learning: in particular feature selection or other typically deterministic components of the process. This
could happen on non-perturbed datasets, and would be much less computationally expensive if it worked.

\subsubsection{Domain Agnostic Applications}
- localizing instabilities within pipelines and correcting them

- Mixed (declining) precision models for pipeline development, such that the stored precision never exceeds the number of
significant digits throughout the evolution of a pipeline.

- Aggregation across perturbations as the new ``mean $\pm$ variance'' for the development of more robust biomarkers

Coming up with error bars of results, not just error bars on statistical tests performed on those results; subsequently
creating a joint false-positive rate as a combined function of the two

Consider the practice of obtaining single estimates of results as favouring bias in the bias-variance trade off, and
instead we should apply confidence-interval logic around our estimates.
(Bias-variance tradeoff https://ieeexplore.ieee.org/document/824819)
(https://www.frontiersin.org/articles/10.3389/fnins.2015.00018/full)

\subsection{20/20}
(find a place to add energy efficiency and awareness of where I compute things 

What is, ``hindsight'', Greg. 

all of these lessons are both for myself and for the community

These recommendations may be in conflict with what the academic incentive structure may encourage, or contrary to what
the ``correct'' decisions may have been when neuroimaging originated. They are not intended to be perfect, merely
lessons that I feel are worth bearing in mind if one were to go start over today.

(example: build clowdr as a part of an established ecosystem, such as nipype)
collaborate, don't compete. build tools as part of supported ecosystems, not in isolation. ``two heads are better
than one'', and the variability of bandwidth overtime makes tools going stale a challenging problem to stay ahead of
unless you're part of a team.

test. this hardly needs to be elaborated upon, but nothing works until you can prove it works

walk, don't run. While it may be fun to sprint through an experiment or field of study, claiming success when we get to
the finish line, it is far more likely that we lead a pack of our colleagues to a dead end than if we slowed down and
checked the map after each step.

Be humble. Be open about our strengths and weaknesses, and collaborate so that we not only all get to spend the most
time using our strength, but we can learn from others around us. Science by definition is interdisciplinary, so we
should admit this and let ourselves shine where we're experts while encouraging others to be the same, collectively
learning and building towards something bigger.

don't become emotionally attached. Too many mistakes happen to loyalty to an idea or bias in interpretting our results
towards an expected outcome. We are human, this is normal, but this is ultimately to our own dismay. Be objective when
you can, and if you can't, call somebody who can be or who expects the opposite result from you so that you can learn
from each other.

Define the objective, and keep it in mind. Too often we fall down rabbit holes and lose touch with the reality of what
we're trying to accomplish. If the goal is academic, define milestones of understanding. If the goal is practical,
define measure of success or failure and timelines for accomplishing them. It becomes far too easy to get swept away in
it all as we pick up momentum, and that does a disservice to ourselves, our funders, and by extension our societies.

