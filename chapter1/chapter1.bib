% Generated by Paperpile. Check out https://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@MANUAL{Kiar2018-lz,
  title     = "A {High-Throughput} Pipeline Identifies Robust Connectomes But
               Troublesome Variability",
  author    = "Kiar, G and Bridgeford, E W and Gray Roncal, W R and
               Chandrashekhar, V and Mhembere, D and Ryman, S and Zuo, X N and
               Marguiles, D S and Craddock, R C and Priebe, C E and Jung, R and
               Calhoun, V D and Caffo, B and Burns, R and Milham, M P and
               Vogelstein, J T",
  abstract  = "Modern scientific discovery depends on collecting large
               heterogeneous datasets with many sources of variability, and on
               applying domain-specific pipelines from which one can draw
               scientific insight or clinical utility. For example, macroscale
               connectomics studies require complex pipelines to process raw
               functional or diffusion data and estimate connectomes.
               Individual studies tend to customize pipelines to their needs,
               raising concerns about their reproducibility, which add to a
               longer list of factors that may differ across studies and result
               in failures to replicate (including sampling, experimental
               design, and data acquisition protocols). Mitigating these issues
               requires the development of pipelines that can be applied across
               multiple studies. We developed
               NeuroData\textbackslashtextquoterights MRI to Graphs (ndmg)
               pipeline using several functional and diffusion studies to
               estimate connectomes. Without any manual intervention or
               parameter tuning, ndmg ran on nearly 30 different studies
               (nearly 6,000 scans), with each scan resulting in a biologically
               plausible connectome (as assessed by multiple quality assurance
               metrics at each processing stage). For each study, the
               connectomes from ndmg are more similar within than across
               individuals, indicating that ndmg is preserving biological
               variability. Moreover, the connectomes exhibit near perfect
               consistency for certain connectional properties across every
               scan, individual, study, site and modality; these include
               stronger ipsilateral than contralateral connections and stronger
               homotopic than heterotopic connections. Yet, how much stronger
               varied across individuals and studies \textbackslashtextemdash
               much more so when pooling data across sites, even after
               controlling for study, site, and basic demographic variables
               (i.e., age, sex, and ethnicity). This indicates that other
               experimental variables (possibly those not measured or reported)
               are contributing to this variability, which if not accounted for
               can limit the value of aggregate datasets, as well as
               expectations regarding the accuracy of findings and likelihood
               of replication. We thus provide a set of principles to guide the
               development of pipelines capable of pooling data across studies
               while maintaining biological variability and minimizing
               measurement error. This open science approach provides us with
               an opportunity to understand and eventually mitigate spurious
               results for both past and future studies.",
  publisher = "Cold Spring Harbor Laboratory",
  year      =  2018,
  keywords  = "gkiarcv.bib"
}

@ARTICLE{Merkel2014-vu,
  title     = "Docker: Lightweight Linux Containers for Consistent Development
               and Deployment",
  author    = "Merkel, Dirk",
  journal   = "Linux J.",
  publisher = "Belltown Media",
  volume    =  2014,
  number    =  239,
  month     =  mar,
  year      =  2014,
  address   = "Houston, TX"
}

@ARTICLE{Open_Science_Collaboration2015-ja,
  title    = "{PSYCHOLOGY}. Estimating the reproducibility of psychological
              science",
  author   = "{Open Science Collaboration}",
  abstract = "Reproducibility is a defining feature of science, but the extent
              to which it characterizes current research is unknown. We
              conducted replications of 100 experimental and correlational
              studies published in three psychology journals using high-powered
              designs and original materials when available. Replication
              effects were half the magnitude of original effects, representing
              a substantial decline. Ninety-seven percent of original studies
              had statistically significant results. Thirty-six percent of
              replications had statistically significant results; 47\% of
              original effect sizes were in the 95\% confidence interval of the
              replication effect size; 39\% of effects were subjectively rated
              to have replicated the original result; and if no bias in
              original results is assumed, combining original and replication
              results left 68\% with statistically significant effects.
              Correlational tests suggest that replication success was better
              predicted by the strength of original evidence than by
              characteristics of the original and replication teams.",
  journal  = "Science",
  volume   =  349,
  number   =  6251,
  pages    = "aac4716",
  month    =  aug,
  year     =  2015,
  language = "en"
}

@UNPUBLISHED{Lampa2018-xn,
  title    = "{SciPipe} - A workflow library for agile development of complex
              and dynamic bioinformatics pipelines",
  author   = "Lampa, Samuel and Dahl{\"o}, Martin and Alvarsson, Jonathan and
              Spjuth, Ola",
  abstract = "Background: The complex nature of biological data has driven the
              development of specialized software tools. Scientific workflow
              management systems simplify the assembly of such tools into
              pipelines and assist with job automation and aids reproducibility
              of analyses. Many contemporary workflow tools are specialized and
              not designed for highly complex workflows, such as with nested
              loops, dynamic scheduling and parametrization, which is common in
              e.g. machine learning. Findings: SciPipe is a workflow
              programming library implemented in the programming language Go,
              for managing complex and dynamic pipelines in bioinformatics,
              cheminformatics and other fields. SciPipe helps in particular
              with workflow constructs common in machine learning, such as
              extensive branching, parameter sweeps and dynamic scheduling and
              parametrization of downstream tasks. SciPipe builds on Flow-based
              programming principles to support agile development of workflows
              based on a library of self-contained, re-usable components. It
              supports running subsets of workflows for improved iterative
              development, and provides a data-centric audit logging feature
              that saves a full audit trace for every output file of a
              workflow, which can be converted to other formats such as HTML,
              TeX and PDF on-demand. The utility of SciPipe is demonstrated
              with a machine learning pipeline, a genomics, and a
              transcriptomics pipeline. Conclusions: SciPipe provides a
              solution for agile development of complex and dynamic pipelines,
              especially in machine leaning, through a flexible programming API
              suitable for scientists used to programming or scripting.",
  journal  = "bioRxiv",
  pages    = "380808",
  month    =  oct,
  year     =  2018,
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Tournier2012-po,
  title     = "{MRtrix}: diffusion tractography in crossing fiber regions",
  author    = "Tournier, J D and Calamante, F and {others}",
  abstract  = "In recent years, diffusion‐weighted magnetic resonance imaging
               has attracted considerable attention due to its unique potential
               to delineate the white matter pathways of the brain. However,
               methodologies currently available and in common use among
               neuroscientists and …",
  journal   = "International Journal of",
  publisher = "Wiley Online Library",
  year      =  2012
}

@PROCEEDINGS{Salari2018-vs,
  title    = "Numerical error propagation in the {HCP} structural
              pre-processing pipelines",
  author   = "Salari, A and Scaria, L and Kiar, G and Glatard, T",
  month    =  jun,
  year     =  2018,
  keywords = "gkiarcv.bib"
}

@ARTICLE{Aleksin2017-ms,
  title    = "{ARACHNE}: A neural-neuroglial network builder with remotely
              controlled parallel computing",
  author   = "Aleksin, Sergey G and Zheng, Kaiyu and Rusakov, Dmitri A and
              Savtchenko, Leonid P",
  abstract = "Creating and running realistic models of neural networks has
              hitherto been a task for computing professionals rather than
              experimental neuroscientists. This is mainly because such
              networks usually engage substantial computational resources, the
              handling of which requires specific programing skills. Here we
              put forward a newly developed simulation environment ARACHNE: it
              enables an investigator to build and explore cellular networks of
              arbitrary biophysical and architectural complexity using the
              logic of NEURON and a simple interface on a local computer or a
              mobile device. The interface can control, through the internet,
              an optimized computational kernel installed on a remote computer
              cluster. ARACHNE can combine neuronal (wired) and astroglial
              (extracellular volume-transmission driven) network types and
              adopt realistic cell models from the NEURON library. The program
              and documentation (current version) are available at GitHub
              repository https://github.com/LeonidSavtchenko/Arachne under the
              MIT License (MIT).",
  journal  = "PLoS Comput. Biol.",
  volume   =  13,
  number   =  3,
  pages    = "e1005467",
  month    =  mar,
  year     =  2017,
  language = "en"
}

@ARTICLE{Reuillon2013-zo,
  title    = "{OpenMOLE}, a workflow engine specifically tailored for the
              distributed exploration of simulation models",
  author   = "Reuillon, Romain and Leclaire, Mathieu and Rey-Coyrehourcq,
              Sebastien",
  abstract = "Complex-systems describe multiple levels of collective structure
              and organization. In such systems, the emergence of global
              behaviour from local interactions is generally studied through
              large scale experiments on numerical models. This analysis
              generates important computation loads which require the use of
              multi-core servers, clusters or grid computing. Dealing with such
              large scale executions is especially challenging for modellers
              who do not possess the theoretical and methodological skills
              required to take advantage of high performance computing
              environments. That is why we have designed a cloud approach for
              model experimentation. This approach has been implemented in
              OpenMOLE (Open MOdeL Experiment) as a Domain Specific Language
              (DSL) that leverages the naturally parallel aspect of model
              experiments. The OpenMOLE DSL has been designed to explore
              user-supplied models. It delegates transparently their numerous
              executions to remote execution environment. From a user
              perspective, those environments are viewed as services providing
              computing power, therefore no technical detail is ever exposed.
              This paper presents the OpenMOLE DSL through the example of a toy
              model exploration and through the automated calibration of a
              real-world complex-system model in the field of geography.",
  journal  = "Future Gener. Comput. Syst.",
  volume   =  29,
  number   =  8,
  pages    = "1981--1990",
  month    =  oct,
  year     =  2013,
  keywords = "Model exploration; Distributed computing; Workflow; Cloud
              computing; Complex-systems"
}

@ARTICLE{Hasham2018-hn,
  title    = "Cloud infrastructure provenance collection and management to
              reproduce scientific workflows execution",
  author   = "Hasham, Khawar and Munir, Kamran and McClatchey, Richard",
  abstract = "The emergence of Cloud computing provides a new computing
              paradigm for scientific workflow execution. It provides dynamic,
              on-demand and scalable resources that enable the processing of
              complex workflow-based experiments. With the ever growing size of
              the experimental data and increasingly complex processing
              workflows, the need for reproducibility has also become
              essential. Provenance has been thought of a mechanism to verify a
              workflow and to provide workflow reproducibility. One of the
              obstacles in reproducing an experiment execution is the lack of
              information about the execution infrastructure in the collected
              provenance. This information becomes critical in the context of
              Cloud in which resources are provisioned on-demand and by
              specifying resource configurations. Therefore, a mechanism is
              required that enables capturing of infrastructure information
              along with the provenance of workflows executing on the Cloud to
              facilitate the re-creation of execution environment on the Cloud.
              This paper presents a framework to Reproduce Scientific Workflow
              Execution using Cloud-Aware Provenance (ReCAP), along with the
              proposed mapping approaches that aid in capturing the Cloud-aware
              provenance information and help in re-provisioning the execution
              resource on the Cloud with similar configurations. Experimental
              evaluation has shown the impact of different resource
              configurations on the workflow execution performance, therefore
              justifies the need for collecting such provenance information in
              the context of Cloud. The evaluation has also demonstrated that
              the proposed mapping approaches can capture Cloud information in
              various Cloud usage scenarios without causing performance
              overhead and can also enable the re-provisioning of resources on
              Cloud. Experiments were conducted using workflows from different
              scientific domains such as astronomy and neuroscience to
              demonstrate the applicability of this research for different
              workflows.",
  journal  = "Future Gener. Comput. Syst.",
  volume   =  86,
  pages    = "799--820",
  month    =  sep,
  year     =  2018,
  keywords = "Scientific workflows; Cloud computing; Cloud infrastructure;
              Provenance; Reproducibility"
}

@ARTICLE{Zuo2014-sj,
  title    = "An open science resource for establishing reliability and
              reproducibility in functional connectomics",
  author   = "Zuo, Xi-Nian and Anderson, Jeffrey S and Bellec, Pierre and Birn,
              Rasmus M and Biswal, Bharat B and Blautzik, Janusch and Breitner,
              John C S and Buckner, Randy L and Calhoun, Vince D and
              Castellanos, F Xavier and Chen, Antao and Chen, Bing and Chen,
              Jiangtao and Chen, Xu and Colcombe, Stanley J and Courtney,
              William and Craddock, R Cameron and Di Martino, Adriana and Dong,
              Hao-Ming and Fu, Xiaolan and Gong, Qiyong and Gorgolewski,
              Krzysztof J and Han, Ying and He, Ye and He, Yong and Ho, Erica
              and Holmes, Avram and Hou, Xiao-Hui and Huckins, Jeremy and
              Jiang, Tianzi and Jiang, Yi and Kelley, William and Kelly, Clare
              and King, Margaret and LaConte, Stephen M and Lainhart, Janet E
              and Lei, Xu and Li, Hui-Jie and Li, Kaiming and Li, Kuncheng and
              Lin, Qixiang and Liu, Dongqiang and Liu, Jia and Liu, Xun and
              Liu, Yijun and Lu, Guangming and Lu, Jie and Luna, Beatriz and
              Luo, Jing and Lurie, Daniel and Mao, Ying and Margulies, Daniel S
              and Mayer, Andrew R and Meindl, Thomas and Meyerand, Mary E and
              Nan, Weizhi and Nielsen, Jared A and O'Connor, David and Paulsen,
              David and Prabhakaran, Vivek and Qi, Zhigang and Qiu, Jiang and
              Shao, Chunhong and Shehzad, Zarrar and Tang, Weijun and
              Villringer, Arno and Wang, Huiling and Wang, Kai and Wei, Dongtao
              and Wei, Gao-Xia and Weng, Xu-Chu and Wu, Xuehai and Xu, Ting and
              Yang, Ning and Yang, Zhi and Zang, Yu-Feng and Zhang, Lei and
              Zhang, Qinglin and Zhang, Zhe and Zhang, Zhiqiang and Zhao, Ke
              and Zhen, Zonglei and Zhou, Yuan and Zhu, Xing-Ting and Milham,
              Michael P",
  abstract = "Efforts to identify meaningful functional imaging-based
              biomarkers are limited by the ability to reliably characterize
              inter-individual differences in human brain function. Although a
              growing number of connectomics-based measures are reported to
              have moderate to high test-retest reliability, the variability in
              data acquisition, experimental designs, and analytic methods
              precludes the ability to generalize results. The Consortium for
              Reliability and Reproducibility (CoRR) is working to address this
              challenge and establish test-retest reliability as a minimum
              standard for methods development in functional connectomics.
              Specifically, CoRR has aggregated 1,629 typical individuals'
              resting state fMRI (rfMRI) data (5,093 rfMRI scans) from 18
              international sites, and is openly sharing them via the
              International Data-sharing Neuroimaging Initiative (INDI). To
              allow researchers to generate various estimates of reliability
              and reproducibility, a variety of data acquisition procedures and
              experimental designs are included. Similarly, to enable users to
              assess the impact of commonly encountered artifacts (for example,
              motion) on characterizations of inter-individual variation,
              datasets of varying quality are included.",
  journal  = "Sci Data",
  volume   =  1,
  pages    = "140049",
  month    =  dec,
  year     =  2014,
  language = "en"
}

@ARTICLE{Bellec2012-um,
  title    = "The pipeline system for Octave and Matlab ({PSOM)}: a lightweight
              scripting framework and execution engine for scientific workflows",
  author   = "Bellec, Pierre and Lavoie-Courchesne, S{\'e}bastien and
              Dickinson, Phil and Lerch, Jason P and Zijdenbos, Alex P and
              Evans, Alan C",
  abstract = "The analysis of neuroimaging databases typically involves a large
              number of inter-connected steps called a pipeline. The pipeline
              system for Octave and Matlab (PSOM) is a flexible framework for
              the implementation of pipelines in the form of Octave or Matlab
              scripts. PSOM does not introduce new language constructs to
              specify the steps and structure of the workflow. All steps of
              analysis are instead described by a regular Matlab data
              structure, documenting their associated command and options, as
              well as their input, output, and cleaned-up files. The PSOM
              execution engine provides a number of automated services: (1) it
              executes jobs in parallel on a local computing facility as long
              as the dependencies between jobs allow for it and sufficient
              resources are available; (2) it generates a comprehensive record
              of the pipeline stages and the history of execution, which is
              detailed enough to fully reproduce the analysis; (3) if an
              analysis is started multiple times, it executes only the parts of
              the pipeline that need to be reprocessed. PSOM is distributed
              under an open-source MIT license and can be used without
              restriction for academic or commercial projects. The package has
              no external dependencies besides Matlab or Octave, is
              straightforward to install and supports of variety of operating
              systems (Linux, Windows, Mac). We ran several benchmark
              experiments on a public database including 200 subjects, using a
              pipeline for the preprocessing of functional magnetic resonance
              images (fMRI). The benchmark results showed that PSOM is a
              powerful solution for the analysis of large databases using local
              or distributed computing resources.",
  journal  = "Front. Neuroinform.",
  volume   =  6,
  pages    = "7",
  month    =  apr,
  year     =  2012,
  keywords = "Matlab; Octave; high-performance computing; neuroimaging;
              open-source; parallel computing; pipeline; workflow",
  language = "en"
}

@ARTICLE{Sochat2016-gl,
  title     = "The Neuroimaging Data Model ({NIDM}) {API}",
  author    = "Sochat, Vanessa and Nichols, B Nolan",
  abstract  = "Sharing of brain research can be aided by the Neuroimaging Data
               Model (NIDM) [1--3]. NIDM provides a community-based framework
               for developing data exchange stand",
  journal   = "Gigascience",
  publisher = "Oxford University Press",
  volume    =  5,
  number    = "suppl\_1",
  pages     = "23--24",
  month     =  nov,
  year      =  2016
}

@ARTICLE{Vivian2017-ul,
  title    = "Toil enables reproducible, open source, big biomedical data
              analyses",
  author   = "Vivian, John and Rao, Arjun Arkal and Nothaft, Frank Austin and
              Ketchum, Christopher and Armstrong, Joel and Novak, Adam and
              Pfeil, Jacob and Narkizian, Jake and Deran, Alden D and
              Musselman-Brown, Audrey and Schmidt, Hannes and Amstutz, Peter
              and Craft, Brian and Goldman, Mary and Rosenbloom, Kate and
              Cline, Melissa and O'Connor, Brian and Hanna, Megan and Birger,
              Chet and Kent, W James and Patterson, David A and Joseph, Anthony
              D and Zhu, Jingchun and Zaranek, Sasha and Getz, Gad and
              Haussler, David and Paten, Benedict",
  journal  = "Nat. Biotechnol.",
  volume   =  35,
  number   =  4,
  pages    = "314--316",
  month    =  apr,
  year     =  2017,
  language = "en"
}

@article{bowring2019exploring,
  title={Exploring the impact of analysis software on task fMRI results},
  author={Bowring, Alexander and Maumet, Camille and Nichols, Thomas E},
  journal={Human brain mapping},
  volume={40},
  number={11},
  pages={3362--3384},
  year={2019},
  publisher={Wiley Online Library}
}

@ARTICLE{Matelsky2018-sy,
  title     = "{Container-Based} Clinical Solutions for Portable and
               Reproducible Image Analysis",
  author    = "Matelsky, J and Kiar, G and Johnson, E and Rivera, C and Toma, M
               and Gray-Roncal, W",
  journal   = "J. Digit. Imaging",
  publisher = "Springer Nature",
  volume    =  31,
  number    =  3,
  pages     = "315--320",
  month     =  may,
  year      =  2018,
  keywords  = "gkiarcv.bib"
}

@MISC{Kiar2016-or,
  title    = "ndmg: {NeuroData's} {MRI} Graphs pipeline",
  author   = "Kiar, G and Gray Roncal, W R and Mhembere, D and Bridgeford, E W
              and Burns, R and Vogelstein, J T",
  journal  = "Zenodo",
  month    =  aug,
  year     =  2016,
  keywords = "gkiarcv.bib"
}

@ARTICLE{Stockton2015-ex,
  title    = "{NeuroManager}: a workflow analysis based simulation management
              engine for computational neuroscience",
  author   = "Stockton, David B and Santamaria, Fidel",
  abstract = "We developed NeuroManager, an object-oriented simulation
              management software engine for computational neuroscience.
              NeuroManager automates the workflow of simulation job submissions
              when using heterogeneous computational resources, simulators, and
              simulation tasks. The object-oriented approach (1) provides
              flexibility to adapt to a variety of neuroscience simulators, (2)
              simplifies the use of heterogeneous computational resources, from
              desktops to super computer clusters, and (3) improves tracking of
              simulator/simulation evolution. We implemented NeuroManager in
              MATLAB, a widely used engineering and scientific language, for
              its signal and image processing tools, prevalence in
              electrophysiology analysis, and increasing use in college Biology
              education. To design and develop NeuroManager we analyzed the
              workflow of simulation submission for a variety of simulators,
              operating systems, and computational resources, including the
              handling of input parameters, data, models, results, and
              analyses. This resulted in 22 stages of simulation submission
              workflow. The software incorporates progress notification,
              automatic organization, labeling, and time-stamping of data and
              results, and integrated access to MATLAB's analysis and
              visualization tools. NeuroManager provides users with the tools
              to automate daily tasks, and assists principal investigators in
              tracking and recreating the evolution of research projects
              performed by multiple people. Overall, NeuroManager provides the
              infrastructure needed to improve workflow, manage multiple
              simultaneous simulations, and maintain provenance of the
              potentially large amounts of data produced during the course of a
              research project.",
  journal  = "Front. Neuroinform.",
  volume   =  9,
  pages    = "24",
  month    =  oct,
  year     =  2015,
  keywords = "MATLAB; NEURON; NeuroML; grid computing; parameter search;
              simulation",
  language = "en"
}

@ARTICLE{Rex2003-pr,
  title    = "The {LONI} Pipeline Processing Environment",
  author   = "Rex, David E and Ma, Jeffrey Q and Toga, Arthur W",
  abstract = "The analysis of raw data in neuroimaging has become a
              computationally entrenched process with many intricate steps run
              on increasingly larger datasets. Many software packages exist
              that provide either complete analyses or specific steps in an
              analysis. These packages often possess diverse input and output
              requirements, utilize different file formats, run in particular
              environments, and have limited abilities with certain types of
              data. The combination of these packages to achieve more sensitive
              and accurate results has become a common tactic in brain mapping
              studies but requires much work to ensure valid interoperation
              between programs. The handling, organization, and storage of
              intermediate data can prove difficult as well. The LONI Pipeline
              Processing Environment is a simple, efficient, and distributed
              computing solution to these problems enabling software inclusion
              from different laboratories in different environments. It is used
              here to derive a T1-weighted MRI atlas of the human brain from
              452 normal young adult subjects with fully automated processing.
              The LONI Pipeline Processing Environment's parallel processing
              efficiency using an integrated client/server dataflow model was
              80.9\% when running the atlas generation pipeline from a PC
              client (Acer TravelMate 340T) on 48 dedicated server processors
              (Silicon Graphics Inc. Origin 3000). The environment was 97.5\%
              efficient when the same analysis was run on eight dedicated
              processors.",
  journal  = "Neuroimage",
  volume   =  19,
  number   =  3,
  pages    = "1033--1048",
  month    =  jul,
  year     =  2003,
  language = "en"
}

@ARTICLE{Andrews2001-dl,
  title    = "Enacting narrative pedagogy. The lived experiences of students
              and teachers",
  author   = "Andrews, C A and Ironside, P M and Nosek, C and Sims, S L and
              Swenson, M M and Yeomans, C and Young, P K and Diekelmann, N",
  abstract = "Reforming nursing education to meet contemporary challenges in
              educational and clinical environments is needed through the
              development and implementation of new pedagogies. Nancy
              Diekelmann is advancing the science of nursing education by
              describing a new phenomenological pedagogy, Narrative Pedagogy,
              identified through interpretive research in nursing education.
              Narrative Pedagogy is an approach to reforming nursing education
              that is always site specific and not generalizable from school to
              school. However, the processes of Narrative Pedagogy are
              transferable and can be enacted in many contexts. This study
              describes the common experiences and shared meanings of teachers
              and students engaging in or enacting Narrative Pedagogy.
              Diekelmann gathered seven teachers and students in five schools
              of nursing in four midwestern states to share their experiences.
              Interpretive phenomenology was used to analyze the group
              interview. One of the findings identified during this analysis,
              Enacting Narrative Pedagogy, is explicated, and two themes,
              Decentering Skill Acquisition and Content and Attending to the
              Practices of Thinking, are described.",
  journal  = "Nurs. Health Care Perspect.",
  volume   =  22,
  number   =  5,
  pages    = "252--259",
  month    =  sep,
  year     =  2001,
  language = "en"
}

@ARTICLE{Glatard2018-uw,
  title    = "Boutiques: a flexible framework to integrate command-line
              applications in computing platforms",
  author   = "Glatard, Tristan and Kiar, Gregory and Aumentado-Armstrong,
              Tristan and Beck, Natacha and Bellec, Pierre and Bernard,
              R{\'e}mi and Bonnet, Axel and Brown, Shawn T and Camarasu-Pop,
              Sorina and Cervenansky, Fr{\'e}d{\'e}ric and Das, Samir and
              Ferreira da Silva, Rafael and Flandin, Guillaume and Girard,
              Pascal and Gorgolewski, Krzysztof J and Guttmann, Charles R G and
              Hayot-Sasson, Val{\'e}rie and Quirion, Pierre-Olivier and Rioux,
              Pierre and Rousseau, Marc-{\'E}tienne and Evans, Alan C",
  abstract = "We present Boutiques, a system to automatically publish,
              integrate, and execute command-line applications across
              computational platforms. Boutiques applications are installed
              through software containers described in a rich and flexible JSON
              language. A set of core tools facilitates the construction,
              validation, import, execution, and publishing of applications.
              Boutiques is currently supported by several distinct virtual
              research platforms, and it has been used to describe dozens of
              applications in the neuroinformatics domain. We expect Boutiques
              to improve the quality of application integration in
              computational platforms, to reduce redundancy of effort, to
              contribute to computational reproducibility, and to foster Open
              Science.",
  journal  = "Gigascience",
  volume   =  7,
  number   =  5,
  month    =  may,
  year     =  2018,
  language = "en"
}

@ARTICLE{Baker2016-en,
  title    = "1,500 scientists lift the lid on reproducibility",
  author   = "Baker, Monya",
  journal  = "Nature",
  volume   =  533,
  number   =  7604,
  pages    = "452--454",
  month    =  may,
  year     =  2016,
  language = "en"
}

@INPROCEEDINGS{Chirigati2016-ep,
  title     = "{ReproZip}: Computational Reproducibility With Ease",
  booktitle = "Proceedings of the 2016 International Conference on Management
               of Data",
  author    = "Chirigati, Fernando and Rampin, R{\'e}mi and Shasha, Dennis and
               Freire, Juliana",
  publisher = "ACM",
  pages     = "2085--2088",
  series    = "SIGMOD '16",
  year      =  2016,
  address   = "New York, NY, USA",
  keywords  = "computational reproducibility, provenance, reprozip",
  location  = "San Francisco, California, USA"
}

@ARTICLE{Eklund2016-wo,
  title    = "Cluster failure: Why {fMRI} inferences for spatial extent have
              inflated false-positive rates",
  author   = "Eklund, Anders and Nichols, Thomas E and Knutsson, Hans",
  abstract = "The most widely used task functional magnetic resonance imaging
              (fMRI) analyses use parametric statistical methods that depend on
              a variety of assumptions. In this work, we use real resting-state
              data and a total of 3 million random task group analyses to
              compute empirical familywise error rates for the fMRI software
              packages SPM, FSL, and AFNI, as well as a nonparametric
              permutation method. For a nominal familywise error rate of 5\%,
              the parametric statistical methods are shown to be conservative
              for voxelwise inference and invalid for clusterwise inference.
              Our results suggest that the principal cause of the invalid
              cluster inferences is spatial autocorrelation functions that do
              not follow the assumed Gaussian shape. By comparison, the
              nonparametric permutation test is found to produce nominal
              results for voxelwise as well as clusterwise inference. These
              findings speak to the need of validating the statistical methods
              being used in the field of neuroimaging.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  113,
  number   =  28,
  pages    = "7900--7905",
  month    =  jul,
  year     =  2016,
  keywords = "cluster inference; fMRI; false positives; permutation test;
              statistics",
  language = "en"
}

@ARTICLE{Deelman2015-ui,
  title    = "Pegasus, a workflow management system for science automation",
  author   = "Deelman, Ewa and Vahi, Karan and Juve, Gideon and Rynge, Mats and
              Callaghan, Scott and Maechling, Philip J and Mayani, Rajiv and
              Chen, Weiwei and Ferreira da Silva, Rafael and Livny, Miron and
              Wenger, Kent",
  abstract = "Modern science often requires the execution of large-scale,
              multi-stage simulation and data analysis pipelines to enable the
              study of complex systems. The amount of computation and data
              involved in these pipelines requires scalable workflow management
              systems that are able to reliably and efficiently coordinate and
              automate data movement and task execution on distributed
              computational resources: campus clusters, national
              cyberinfrastructures, and commercial and academic clouds. This
              paper describes the design, development and evolution of the
              Pegasus Workflow Management System, which maps abstract workflow
              descriptions onto distributed computing infrastructures. Pegasus
              has been used for more than twelve years by scientists in a wide
              variety of domains, including astronomy, seismology,
              bioinformatics, physics and others. This paper provides an
              integrated view of the Pegasus system, showing its capabilities
              that have been developed over time in response to application
              needs and to the evolution of the scientific computing platforms.
              The paper describes how Pegasus achieves reliable, scalable
              workflow execution across a wide variety of computing
              infrastructures.",
  journal  = "Future Gener. Comput. Syst.",
  volume   =  46,
  pages    = "17--35",
  month    =  may,
  year     =  2015,
  keywords = "Scientific workflows; Workflow management system; Pegasus"
}

@ARTICLE{Gorgolewski2011-pq,
  title    = "Nipype: a flexible, lightweight and extensible neuroimaging data
              processing framework in python",
  author   = "Gorgolewski, Krzysztof and Burns, Christopher D and Madison,
              Cindee and Clark, Dav and Halchenko, Yaroslav O and Waskom,
              Michael L and Ghosh, Satrajit S",
  abstract = "Current neuroimaging software offer users an incredible
              opportunity to analyze their data in different ways, with
              different underlying assumptions. Several sophisticated software
              packages (e.g., AFNI, BrainVoyager, FSL, FreeSurfer, Nipy, R,
              SPM) are used to process and analyze large and often diverse
              (highly multi-dimensional) data. However, this heterogeneous
              collection of specialized applications creates several issues
              that hinder replicable, efficient, and optimal use of
              neuroimaging analysis approaches: (1) No uniform access to
              neuroimaging analysis software and usage information; (2) No
              framework for comparative algorithm development and
              dissemination; (3) Personnel turnover in laboratories often
              limits methodological continuity and training new personnel takes
              time; (4) Neuroimaging software packages do not address
              computational efficiency; and (5) Methods sections in journal
              articles are inadequate for reproducing results. To address these
              issues, we present Nipype (Neuroimaging in Python: Pipelines and
              Interfaces; http://nipy.org/nipype), an open-source,
              community-developed, software package, and scriptable library.
              Nipype solves the issues by providing Interfaces to existing
              neuroimaging software with uniform usage semantics and by
              facilitating interaction between these packages using Workflows.
              Nipype provides an environment that encourages interactive
              exploration of algorithms, eases the design of Workflows within
              and between packages, allows rapid comparative development of
              algorithms and reduces the learning curve necessary to use
              different packages. Nipype supports both local and remote
              execution on multi-core machines and clusters, without additional
              scripting. Nipype is Berkeley Software Distribution licensed,
              allowing anyone unrestricted usage. An open, community-driven
              development philosophy allows the software to quickly adapt and
              address the varied needs of the evolving neuroimaging community,
              especially in the context of increasing demand for reproducible
              research.",
  journal  = "Front. Neuroinform.",
  volume   =  5,
  pages    = "13",
  month    =  aug,
  year     =  2011,
  keywords = "Python; data processing; neuroimaging; pipeline; reproducible
              research; workflow",
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Chirigati2013-bu,
  title     = "{ReproZip}: Using Provenance to Support Computational
               Reproducibility",
  author    = "Chirigati, F S and Shasha, D E and Freire, J",
  abstract  = "We describe ReproZip , a tool that makes it easier for authors
               to publish reproducible results and for reviewers to validate
               these results. By tracking operating system calls, ReproZip
               systematically captures detailed provenance of existing
               experiments, including data …",
  journal   = "Tappi J.",
  publisher = "usenix.org",
  year      =  2013
}

@ARTICLE{Reunanen2003-fr,
  title    = "Overfitting in Making Comparisons Between Variable Selection
              Methods",
  author   = "Reunanen, Juha",
  journal  = "J. Mach. Learn. Res.",
  volume   =  3,
  number   = "Mar",
  pages    = "1371--1382",
  year     =  2003
}

@MISC{Glatard2017-nc,
  title    = "Boutiques: A descriptive command-line framework",
  author   = "Glatard, T and Kiar, G and Aumentado-Armstrong, T and Beck, N and
              Ferreira da Silva, R and Rousseau, M E",
  journal  = "Zenodo",
  month    =  sep,
  year     =  2017,
  keywords = "gkiarcv.bib"
}

@ARTICLE{Cramer2003-qz,
  title    = "{HIF-1$\alpha$} Is Essential for Myeloid {Cell-Mediated}
              Inflammation",
  author   = "Cramer, Thorsten and Yamanishi, Yuji and Clausen, Bj{\"o}rn E and
              F{\"o}rster, Irmgard and Pawlinski, Rafal and Mackman, Nigel and
              Haase, Volker H and Jaenisch, Rudolf and Corr, Maripat and Nizet,
              Victor and Firestein, Gary S and Gerber, Hans-Peter and Ferrara,
              Napoleone and Johnson, Randall S",
  abstract = "Granulocytes and monocytes/macrophages of the myeloid lineage are
              the chief cellular agents of innate immunity. Here, we have
              examined the inflammatory response in mice with conditional
              knockouts of the hypoxia responsive transcription factor
              HIF-1$\alpha$, its negative regulator VHL, and a known downstream
              target, VEGF. We find that activation of HIF-1$\alpha$ is
              essential for myeloid cell infiltration and activation in vivo
              through a mechanism independent of VEGF. Loss of VHL leads to a
              large increase in acute inflammatory responses. Our results show
              that HIF-1$\alpha$ is essential for the regulation of glycolytic
              capacity in myeloid cells: when HIF-1$\alpha$ is absent, the
              cellular ATP pool is drastically reduced. The metabolic defect
              results in profound impairment of myeloid cell aggregation,
              motility, invasiveness, and bacterial killing. This role for
              HIF-1$\alpha$ demonstrates its direct regulation of survival and
              function in the inflammatory microenvironment.",
  journal  = "Cell",
  volume   =  112,
  number   =  5,
  pages    = "645--657",
  month    =  mar,
  year     =  2003
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Bellantuono2012-et,
  title    = "Coral thermal tolerance: tuning gene expression to resist thermal
              stress",
  author   = "Bellantuono, Anthony J and Granados-Cifuentes, Camila and Miller,
              David J and Hoegh-Guldberg, Ove and Rodriguez-Lanetty, Mauricio",
  abstract = "The acclimatization capacity of corals is a critical
              consideration in the persistence of coral reefs under stresses
              imposed by global climate change. The stress history of corals
              plays a role in subsequent response to heat stress, but the
              transcriptomic changes associated with these plastic changes have
              not been previously explored. In order to identify host
              transcriptomic changes associated with acquired thermal tolerance
              in the scleractinian coral Acropora millepora, corals
              preconditioned to a sub-lethal temperature of 3°C below bleaching
              threshold temperature were compared to both non-preconditioned
              corals and untreated controls using a cDNA microarray platform.
              After eight days of hyperthermal challenge, conditions under
              which non-preconditioned corals bleached and preconditioned
              corals (thermal-tolerant) maintained Symbiodinium density, a
              clear differentiation in the transcriptional profiles was
              revealed among the condition examined. Among these changes, nine
              differentially expressed genes separated preconditioned corals
              from non-preconditioned corals, with 42 genes differentially
              expressed between control and preconditioned treatments, and 70
              genes between non-preconditioned corals and controls.
              Differentially expressed genes included components of an
              apoptotic signaling cascade, which suggest the inhibition of
              apoptosis in preconditioned corals. Additionally, lectins and
              genes involved in response to oxidative stress were also
              detected. One dominant pattern was the apparent tuning of gene
              expression observed between preconditioned and non-preconditioned
              treatments; that is, differences in expression magnitude were
              more apparent than differences in the identity of genes
              differentially expressed. Our work revealed a transcriptomic
              signature underlying the tolerance associated with coral thermal
              history, and suggests that understanding the molecular mechanisms
              behind physiological acclimatization would be critical for the
              modeling of reefs in impending climate change scenarios.",
  journal  = "PLoS One",
  volume   =  7,
  number   =  11,
  pages    = "e50685",
  month    =  nov,
  year     =  2012,
  language = "en"
}

@INPROCEEDINGS{Missier2013-ml,
  title     = "The {W3C} {PROV} Family of Specifications for Modelling
               Provenance Metadata",
  booktitle = "Proceedings of the 16th International Conference on Extending
               Database Technology",
  author    = "Missier, Paolo and Belhajjame, Khalid and Cheney, James",
  publisher = "ACM",
  pages     = "773--776",
  series    = "EDBT '13",
  year      =  2013,
  address   = "New York, NY, USA",
  location  = "Genoa, Italy"
}

@ARTICLE{Sudlow2015-dl,
  title    = "{UK} biobank: an open access resource for identifying the causes
              of a wide range of complex diseases of middle and old age",
  author   = "Sudlow, Cathie and Gallacher, John and Allen, Naomi and Beral,
              Valerie and Burton, Paul and Danesh, John and Downey, Paul and
              Elliott, Paul and Green, Jane and Landray, Martin and Liu, Bette
              and Matthews, Paul and Ong, Giok and Pell, Jill and Silman, Alan
              and Young, Alan and Sprosen, Tim and Peakman, Tim and Collins,
              Rory",
  abstract = "Cathie Sudlow and colleagues describe the UK Biobank, a large
              population-based prospective study, established to allow
              investigation of the genetic and non-genetic determinants of the
              diseases of middle and old age.",
  journal  = "PLoS Med.",
  volume   =  12,
  number   =  3,
  pages    = "e1001779",
  month    =  mar,
  year     =  2015,
  language = "en"
}

@ARTICLE{Kiar2017-cn,
  title    = "Science in the cloud ({SIC)}: A use case in {MRI} connectomics",
  author   = "Kiar, Gregory and Gorgolewski, Krzysztof J and Kleissas, Dean and
              Roncal, William Gray and Litt, Brian and Wandell, Brian and
              Poldrack, Russel A and Wiener, Martin and Vogelstein, R Jacob and
              Burns, Randal and Vogelstein, Joshua T",
  abstract = "Modern technologies are enabling scientists to collect
              extraordinary amounts of complex and sophisticated data across a
              huge range of scales like never before. With this onslaught of
              data, we can allow the focal point to shift from data collection
              to data analysis. Unfortunately, lack of standardized sharing
              mechanisms and practices often make reproducing or extending
              scientific results very difficult. With the creation of data
              organization structures and tools that drastically improve code
              portability, we now have the opportunity to design such a
              framework for communicating extensible scientific discoveries.
              Our proposed solution leverages these existing technologies and
              standards, and provides an accessible and extensible model for
              reproducible research, called 'science in the cloud' (SIC).
              Exploiting scientific containers, cloud computing, and cloud data
              services, we show the capability to compute in the cloud and run
              a web service that enables intimate interaction with the tools
              and data presented. We hope this model will inspire the community
              to produce reproducible and, importantly, extensible results that
              will enable us to collectively accelerate the rate at which
              scientific breakthroughs are discovered, replicated, and
              extended.",
  journal  = "Gigascience",
  volume   =  6,
  number   =  5,
  pages    = "1--10",
  month    =  may,
  year     =  2017,
  keywords = "Cloud Computing; Connectomics; MRI; Reproducibility",
  language = "en"
}

@ARTICLE{Gorgolewski2017-sr,
  title     = "{BIDS} apps: Improving ease of use, accessibility, and
               reproducibility of neuroimaging data analysis methods",
  author    = "Gorgolewski, Krzysztof J and Alfaro-Almagro, Fidel and Auer,
               Tibor and Bellec, Pierre and Capot{\u a}, Mihai and Chakravarty,
               M Mallar and Churchill, Nathan W and Cohen, Alexander Li and
               Craddock, R Cameron and Devenyi, Gabriel A and {Others}",
  journal   = "PLoS Comput. Biol.",
  publisher = "Public Library of Science",
  volume    =  13,
  number    =  3,
  pages     = "e1005209",
  year      =  2017
}

@ARTICLE{Gorgolewski2016-om,
  title    = "The brain imaging data structure, a format for organizing and
              describing outputs of neuroimaging experiments",
  author   = "Gorgolewski, Krzysztof J and Auer, Tibor and Calhoun, Vince D and
              Craddock, R Cameron and Das, Samir and Duff, Eugene P and
              Flandin, Guillaume and Ghosh, Satrajit S and Glatard, Tristan and
              Halchenko, Yaroslav O and Handwerker, Daniel A and Hanke, Michael
              and Keator, David and Li, Xiangrui and Michael, Zachary and
              Maumet, Camille and Nichols, B Nolan and Nichols, Thomas E and
              Pellman, John and Poline, Jean-Baptiste and Rokem, Ariel and
              Schaefer, Gunnar and Sochat, Vanessa and Triplett, William and
              Turner, Jessica A and Varoquaux, Ga{\"e}l and Poldrack, Russell A",
  abstract = "The development of magnetic resonance imaging (MRI) techniques
              has defined modern neuroimaging. Since its inception, tens of
              thousands of studies using techniques such as functional MRI and
              diffusion weighted imaging have allowed for the non-invasive
              study of the brain. Despite the fact that MRI is routinely used
              to obtain data for neuroscience research, there has been no
              widely adopted standard for organizing and describing the data
              collected in an imaging experiment. This renders sharing and
              reusing data (within or between labs) difficult if not impossible
              and unnecessarily complicates the application of automatic
              pipelines and quality assurance protocols. To solve this problem,
              we have developed the Brain Imaging Data Structure (BIDS), a
              standard for organizing and describing MRI datasets. The BIDS
              standard uses file formats compatible with existing software,
              unifies the majority of practices already common in the field,
              and captures the metadata necessary for most common data
              processing operations.",
  journal  = "Sci Data",
  volume   =  3,
  pages    = "160044",
  month    =  jun,
  year     =  2016,
  language = "en"
}

@ARTICLE{Meyer2016-zt,
  title    = "pypet: A Python Toolkit for Data Management of Parameter
              Explorations",
  author   = "Meyer, Robert and Obermayer, Klaus",
  abstract = "pypet (Python parameter exploration toolkit) is a new
              multi-platform Python toolkit for managing numerical simulations.
              Sampling the space of model parameters is a key aspect of
              simulations and numerical experiments. pypet is designed to allow
              easy and arbitrary sampling of trajectories through a parameter
              space beyond simple grid searches. pypet collects and stores both
              simulation parameters and results in a single HDF5 file. This
              collective storage allows fast and convenient loading of data for
              further analyses. pypet provides various additional features such
              as multiprocessing and parallelization of simulations, dynamic
              loading of data, integration of git version control, and
              supervision of experiments via the electronic lab notebook
              Sumatra. pypet supports a rich set of data formats, including
              native Python types, Numpy and Scipy data, Pandas DataFrames, and
              BRIAN(2) quantities. Besides these formats, users can easily
              extend the toolkit to allow customized data types. pypet is a
              flexible tool suited for both short Python scripts and large
              scale projects. pypet's various features, especially the tight
              link between parameters and results, promote reproducible
              research in computational neuroscience and simulation-based
              disciplines.",
  journal  = "Front. Neuroinform.",
  volume   =  10,
  pages    = "38",
  month    =  aug,
  year     =  2016,
  keywords = "grid computing; parallelization; parameter exploration; python;
              reproducibility; simulation",
  language = "en"
}

@MISC{Kiar2018-lq,
  title        = "Clowdr: Accessible pipeline deployment and sharing",
  author       = "Kiar, G",
  journal      = "Zenodo",
  month        =  mar,
  year         =  2018,
  howpublished = "Zenodo",
  keywords     = "gkiarcv.bib"
}

@ARTICLE{Hines2001-wo,
  title    = "{NEURON}: a tool for neuroscientists",
  author   = "Hines, M L and Carnevale, N T",
  abstract = "NEURON is a simulation environment for models of individual
              neurons and networks of neurons that are closely linked to
              experimental data. NEURON provides tools for conveniently
              constructing, exercising, and managing models, so that special
              expertise in numerical methods or programming is not required for
              its productive use. This article describes two tools that address
              the problem of how to achieve computational efficiency and
              accuracy.",
  journal  = "Neuroscientist",
  volume   =  7,
  number   =  2,
  pages    = "123--135",
  month    =  apr,
  year     =  2001,
  language = "en"
}

@ARTICLE{Milkowski2018-je,
  title    = "Replicability or reproducibility? On the replication crisis in
              computational neuroscience and sharing only relevant detail",
  author   = "Mi{\l}kowski, Marcin and Hensel, Witold M and Hohol, Mateusz",
  abstract = "Replicability and reproducibility of computational models has
              been somewhat understudied by ``the replication movement.'' In
              this paper, we draw on methodological studies into the
              replicability of psychological experiments and on the mechanistic
              account of explanation to analyze the functions of model
              replications and model reproductions in computational
              neuroscience. We contend that model replicability, or independent
              researchers' ability to obtain the same output using original
              code and data, and model reproducibility, or independent
              researchers' ability to recreate a model without original code,
              serve different functions and fail for different reasons. This
              means that measures designed to improve model replicability may
              not enhance (and, in some cases, may actually damage) model
              reproducibility. We claim that although both are undesirable, low
              model reproducibility poses more of a threat to long-term
              scientific progress than low model replicability. In our opinion,
              low model reproducibility stems mostly from authors' omitting to
              provide crucial information in scientific papers and we stress
              that sharing all computer code and data is not a solution.
              Reports of computational studies should remain selective and
              include all and only relevant bits of code.",
  journal  = "J. Comput. Neurosci.",
  month    =  oct,
  year     =  2018,
  keywords = "Computational modeling; Direct and conceptual replication;
              Methodology of computational neuroscience; Replication and
              reproduction; Replication studies",
  language = "en"
}

@ARTICLE{Bui2015-pd,
  title         = "Analysis of Docker Security",
  author        = "Bui, Thanh",
  abstract      = "Over the last few years, the use of virtualization
                   technologies has increased dramatically. This makes the
                   demand for efficient and secure virtualization solutions
                   become more obvious. Container-based virtualization and
                   hypervisor-based virtualization are two main types of
                   virtualization technologies that have emerged to the market.
                   Of these two classes, container-based virtualization is able
                   to provide a more lightweight and efficient virtual
                   environment, but not without security concerns. In this
                   paper, we analyze the security level of Docker, a well-known
                   representative of container-based approaches. The analysis
                   considers two areas: (1) the internal security of Docker,
                   and (2) how Docker interacts with the security features of
                   the Linux kernel, such as SELinux and AppArmor, in order to
                   harden the host system. Furthermore, the paper also
                   discusses and identifies what could be done when using
                   Docker to increase its level of security.",
  month         =  jan,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CR",
  eprint        = "1501.02967"
}

@ARTICLE{Smith2004-ot,
  title    = "Advances in functional and structural {MR} image analysis and
              implementation as {FSL}",
  author   = "Smith, Stephen M and Jenkinson, Mark and Woolrich, Mark W and
              Beckmann, Christian F and Behrens, Timothy E J and Johansen-Berg,
              Heidi and Bannister, Peter R and De Luca, Marilena and Drobnjak,
              Ivana and Flitney, David E and Niazy, Rami K and Saunders, James
              and Vickers, John and Zhang, Yongyue and De Stefano, Nicola and
              Brady, J Michael and Matthews, Paul M",
  abstract = "The techniques available for the interrogation and analysis of
              neuroimaging data have a large influence in determining the
              flexibility, sensitivity, and scope of neuroimaging experiments.
              The development of such methodologies has allowed investigators
              to address scientific questions that could not previously be
              answered and, as such, has become an important research area in
              its own right. In this paper, we present a review of the research
              carried out by the Analysis Group at the Oxford Centre for
              Functional MRI of the Brain (FMRIB). This research has focussed
              on the development of new methodologies for the analysis of both
              structural and functional magnetic resonance imaging data. The
              majority of the research laid out in this paper has been
              implemented as freely available software tools within FMRIB's
              Software Library (FSL).",
  journal  = "Neuroimage",
  volume   = "23 Suppl 1",
  pages    = "S208--19",
  year     =  2004,
  language = "en"
}

@ARTICLE{Jenkinson2012-ly,
  title    = "{FSL}",
  author   = "Jenkinson, Mark and Beckmann, Christian F and Behrens, Timothy E
              J and Woolrich, Mark W and Smith, Stephen M",
  abstract = "FSL (the FMRIB Software Library) is a comprehensive library of
              analysis tools for functional, structural and diffusion MRI brain
              imaging data, written mainly by members of the Analysis Group,
              FMRIB, Oxford. For this NeuroImage special issue on ``20 years of
              fMRI'' we have been asked to write about the history,
              developments and current status of FSL. We also include some
              descriptions of parts of FSL that are not well covered in the
              existing literature. We hope that some of this content might be
              of interest to users of FSL, and also maybe to new research
              groups considering creating, releasing and supporting new
              software packages for brain image analysis.",
  journal  = "Neuroimage",
  volume   =  62,
  number   =  2,
  pages    = "782--790",
  month    =  aug,
  year     =  2012,
  language = "en"
}

@ARTICLE{Sherif2014-ve,
  title    = "{CBRAIN}: a web-based, distributed computing platform for
              collaborative neuroimaging research",
  author   = "Sherif, Tarek and Rioux, Pierre and Rousseau, Marc-Etienne and
              Kassis, Nicolas and Beck, Natacha and Adalat, Reza and Das, Samir
              and Glatard, Tristan and Evans, Alan C",
  abstract = "The Canadian Brain Imaging Research Platform (CBRAIN) is a
              web-based collaborative research platform developed in response
              to the challenges raised by data-heavy, compute-intensive
              neuroimaging research. CBRAIN offers transparent access to remote
              data sources, distributed computing sites, and an array of
              processing and visualization tools within a controlled, secure
              environment. Its web interface is accessible through any modern
              browser and uses graphical interface idioms to reduce the
              technical expertise required to perform large-scale computational
              analyses. CBRAIN's flexible meta-scheduling has allowed the
              incorporation of a wide range of heterogeneous computing sites,
              currently including nine national research High Performance
              Computing (HPC) centers in Canada, one in Korea, one in Germany,
              and several local research servers. CBRAIN leverages remote
              computing cycles and facilitates resource-interoperability in a
              transparent manner for the end-user. Compared with typical grid
              solutions available, our architecture was designed to be easily
              extendable and deployed on existing remote computing sites with
              no tool modification, administrative intervention, or special
              software/hardware configuration. As October 2013, CBRAIN serves
              over 200 users spread across 53 cities in 17 countries. The
              platform is built as a generic framework that can accept data and
              analysis tools from any discipline. However, its current focus is
              primarily on neuroimaging research and studies of neurological
              diseases such as Autism, Parkinson's and Alzheimer's diseases,
              Multiple Sclerosis as well as on normal brain structure and
              development. This technical report presents the CBRAIN Platform,
              its current deployment and usage and future direction.",
  journal  = "Front. Neuroinform.",
  volume   =  8,
  pages    = "54",
  month    =  may,
  year     =  2014,
  keywords = "cloud computing; collaborative platform; distributed computing;
              eScience; interoperability; meta-scheduler; neuroimaging;
              visualization",
  language = "en"
}

@ARTICLE{Rosenthal1979-ic,
  title     = "The file drawer problem and tolerance for null results",
  author    = "Rosenthal, Robert",
  journal   = "Psychol. Bull.",
  publisher = "American Psychological Association",
  volume    =  86,
  number    =  3,
  pages     = "638",
  year      =  1979
}

@ARTICLE{Kurtzer2017-kq,
  title    = "Singularity: Scientific containers for mobility of compute",
  author   = "Kurtzer, Gregory M and Sochat, Vanessa and Bauer, Michael W",
  abstract = "Here we present Singularity, software developed to bring
              containers and reproducibility to scientific computing. Using
              Singularity containers, developers can work in reproducible
              environments of their choosing and design, and these complete
              environments can easily be copied and executed on other
              platforms. Singularity is an open source initiative that
              harnesses the expertise of system and software engineers and
              researchers alike, and integrates seamlessly into common
              workflows for both of these groups. As its primary use case,
              Singularity brings mobility of computing to both users and HPC
              centers, providing a secure means to capture and distribute
              software and compute environments. This ability to create and
              deploy reproducible environments across these centers, a
              previously unmet need, makes Singularity a game changing
              development for computational science.",
  journal  = "PLoS One",
  volume   =  12,
  number   =  5,
  pages    = "e0177459",
  month    =  may,
  year     =  2017,
  language = "en"
}

@ARTICLE{Dinov2010-yq,
  title    = "Neuroimaging study designs, computational analyses and data
              provenance using the {LONI} pipeline",
  author   = "Dinov, Ivo and Lozev, Kamen and Petrosyan, Petros and Liu,
              Zhizhong and Eggert, Paul and Pierce, Jonathan and Zamanyan, Alen
              and Chakrapani, Shruthi and Van Horn, John and Parker, D Stott
              and Magsipoc, Rico and Leung, Kelvin and Gutman, Boris and Woods,
              Roger and Toga, Arthur",
  abstract = "Modern computational neuroscience employs diverse software tools
              and multidisciplinary expertise to analyze heterogeneous brain
              data. The classical problems of gathering meaningful data,
              fitting specific models, and discovering appropriate analysis and
              visualization tools give way to a new class of computational
              challenges--management of large and incongruous data, integration
              and interoperability of computational resources, and data
              provenance. We designed, implemented and validated a new paradigm
              for addressing these challenges in the neuroimaging field. Our
              solution is based on the LONI Pipeline environment [3], [4], a
              graphical workflow environment for constructing and executing
              complex data processing protocols. We developed study-design,
              database and visual language programming functionalities within
              the LONI Pipeline that enable the construction of complete,
              elaborate and robust graphical workflows for analyzing
              neuroimaging and other data. These workflows facilitate open
              sharing and communication of data and metadata, concrete
              processing protocols, result validation, and study replication
              among different investigators and research groups. The LONI
              Pipeline features include distributed grid-enabled
              infrastructure, virtualized execution environment, efficient
              integration, data provenance, validation and distribution of new
              computational tools, automated data format conversion, and an
              intuitive graphical user interface. We demonstrate the new LONI
              Pipeline features using large scale neuroimaging studies based on
              data from the International Consortium for Brain Mapping [5] and
              the Alzheimer's Disease Neuroimaging Initiative [6]. User guides,
              forums, instructions and downloads of the LONI Pipeline
              environment are available at http://pipeline.loni.ucla.edu.",
  journal  = "PLoS One",
  volume   =  5,
  number   =  9,
  month    =  sep,
  year     =  2010,
  language = "en"
}

@ARTICLE{Cox2004-qw,
  title     = "\textbf{A (Sort of) New Image Data Format Standard: {NIfTI-1}}:
               {\textbf{WE} 150}",
  author    = "Cox, Robert W and Ashburner, John and Breman, Hester and
               Fissell, Kate and Haselgrove, Christian and Holmes, Colin J and
               Lancaster, Jack L and Rex, David E and Smith, Stephen M and
               Woodward, Jeffrey B and Strother, Stephen C",
  journal   = "Neuroimage",
  publisher = "Neuroimage",
  volume    =  22,
  pages     = "e1440",
  month     =  jun,
  year      =  2004,
  language  = "en"
}

@ARTICLE{Poldrack2013-wi,
  title    = "Toward open sharing of task-based {fMRI} data: the {OpenfMRI}
              project",
  author   = "Poldrack, Russell A and Barch, Deanna M and Mitchell, Jason P and
              Wager, Tor D and Wagner, Anthony D and Devlin, Joseph T and
              Cumba, Chad and Koyejo, Oluwasanmi and Milham, Michael P",
  abstract = "The large-scale sharing of task-based functional neuroimaging
              data has the potential to allow novel insights into the
              organization of mental function in the brain, but the field of
              neuroimaging has lagged behind other areas of bioscience in the
              development of data sharing resources. This paper describes the
              OpenFMRI project (accessible online at http://www.openfmri.org),
              which aims to provide the neuroimaging community with a resource
              to support open sharing of task-based fMRI studies. We describe
              the motivation behind the project, focusing particularly on how
              this project addresses some of the well-known challenges to
              sharing of task-based fMRI data. Results from a preliminary
              analysis of the current database are presented, which demonstrate
              the ability to classify between task contrasts with high
              generalization accuracy across subjects, and the ability to
              identify individual subjects from their activation maps with
              moderately high accuracy. Clustering analyses show that the
              similarity relations between statistical maps have a somewhat
              orderly relation to the mental functions engaged by the relevant
              tasks. These results highlight the potential of the project to
              support large-scale multivariate analyses of the relation between
              mental processes and brain function.",
  journal  = "Front. Neuroinform.",
  volume   =  7,
  pages    = "12",
  month    =  jul,
  year     =  2013,
  keywords = "classification; data sharing; informatics; metadata; multivariate",
  language = "en"
}

@ARTICLE{Goecks2010-cl,
  title    = "Galaxy: a comprehensive approach for supporting accessible,
              reproducible, and transparent computational research in the life
              sciences",
  author   = "Goecks, Jeremy and Nekrutenko, Anton and Taylor, James and
              {Galaxy Team}",
  abstract = "Increased reliance on computational approaches in the life
              sciences has revealed grave concerns about how accessible and
              reproducible computation-reliant results truly are. Galaxy
              http://usegalaxy.org, an open web-based platform for genomic
              research, addresses these problems. Galaxy automatically tracks
              and manages data provenance and provides support for capturing
              the context and intent of computational methods. Galaxy Pages are
              interactive, web-based documents that provide users with a medium
              to communicate a complete computational analysis.",
  journal  = "Genome Biol.",
  volume   =  11,
  number   =  8,
  pages    = "R86",
  month    =  aug,
  year     =  2010,
  language = "en"
}

@ARTICLE{Van_Essen2013-bx,
  title    = "The {WU-Minn} Human Connectome Project: an overview",
  author   = "Van Essen, David C and Smith, Stephen M and Barch, Deanna M and
              Behrens, Timothy E J and Yacoub, Essa and Ugurbil, Kamil and
              {WU-Minn HCP Consortium}",
  abstract = "The Human Connectome Project consortium led by Washington
              University, University of Minnesota, and Oxford University is
              undertaking a systematic effort to map macroscopic human brain
              circuits and their relationship to behavior in a large population
              of healthy adults. This overview article focuses on progress made
              during the first half of the 5-year project in refining the
              methods for data acquisition and analysis. Preliminary analyses
              based on a finalized set of acquisition and preprocessing
              protocols demonstrate the exceptionally high quality of the data
              from each modality. The first quarterly release of imaging and
              behavioral data via the ConnectomeDB database demonstrates the
              commitment to making HCP datasets freely accessible. Altogether,
              the progress to date provides grounds for optimism that the HCP
              datasets and associated methods and software will become
              increasingly valuable resources for characterizing human brain
              connectivity and function, their relationship to behavior, and
              their heritability and genetic underpinnings.",
  journal  = "Neuroimage",
  volume   =  80,
  pages    = "62--79",
  month    =  oct,
  year     =  2013,
  language = "en"
}

@ARTICLE{Combe2016-vb,
  title    = "To Docker or Not to Docker: A Security Perspective",
  author   = "Combe, T and Martin, A and Pietro, R Di",
  abstract = "The need for ever-shorter development cycles, continuous
              delivery, and cost savings in cloud-based infrastructures led to
              the rise of containers, which are more flexible than virtual
              machines and provide near-native performance. Among all container
              solutions, Docker, a complete packaging and software delivery
              tool, currently leads the market. This article gives an overview
              of the container ecosystem and discusses the Docker environment's
              security implications through realistic use cases. The authors
              define an adversary model, point out several vulnerabilities
              affecting current Docker usage, and discuss further research
              directions.",
  journal  = "IEEE Cloud Computing",
  volume   =  3,
  number   =  5,
  pages    = "54--62",
  year     =  2016,
  keywords = "cloud computing;security of data;software tools;security
              perspective;cloud-based infrastructures;packaging tool;software
              delivery tool;container ecosystem;Docker environment
              security;Product life cycle management;Computer security;Cost
              benefit analysis;Cloud computing;Linux;Containers;Virtual
              networks;cloud
              computing;virtualization;containers;Docker;security"
}

@INPROCEEDINGS{Rocklin2015-na,
  title       = "Dask: Parallel computation with blocked algorithms and task
                 scheduling",
  booktitle   = "Proceedings of the 14th Python in Science Conference",
  author      = "Rocklin, Matthew",
  institution = "Citeseer",
  year        =  2015
}
